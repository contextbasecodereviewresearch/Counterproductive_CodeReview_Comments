{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQUVFyI58Hnb"
   },
   "source": [
    "**reading data and set them into pandas df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3508,
     "status": "ok",
     "timestamp": 1701591969073,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "qLFF15YI8HUR",
    "outputId": "ef0a3024-e205-461f-f5c7-7857f7cd2d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1701591972656,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "V_KgS6dq9JAi",
    "outputId": "4c0a426a-ad7b-4d3a-8051-6f0b563b13f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>Personal attacks</th>\n",
       "      <th>Threats or intimidation</th>\n",
       "      <th>Mockery</th>\n",
       "      <th>Lack of specificity</th>\n",
       "      <th>Discouragement without guide</th>\n",
       "      <th>Disregard for other time or boundaries</th>\n",
       "      <th>Unconscious bias</th>\n",
       "      <th>Dismissive attitude</th>\n",
       "      <th>Excessive control</th>\n",
       "      <th>Social</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>AntiSocial</th>\n",
       "      <th>NonToxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/It may happen that a service die/A service ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@zhiyan, thanks for helping explanation. Overa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all the code you have inline below should be r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All you do in the interrupt handler is call wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are you sure this leads to a color that makes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  Personal attacks  \\\n",
       "0  /It may happen that a service die/A service ma...                 0   \n",
       "1  @zhiyan, thanks for helping explanation. Overa...                 0   \n",
       "2  all the code you have inline below should be r...                 0   \n",
       "3  All you do in the interrupt handler is call wa...                 1   \n",
       "4  Are you sure this leads to a color that makes ...                 0   \n",
       "\n",
       "   Threats or intimidation  Mockery  Lack of specificity  \\\n",
       "0                        0        0                    1   \n",
       "1                        0        0                    0   \n",
       "2                        0        0                    0   \n",
       "3                        0        1                    1   \n",
       "4                        0        0                    0   \n",
       "\n",
       "   Discouragement without guide  Disregard for other time or boundaries  \\\n",
       "0                             0                                       0   \n",
       "1                             0                                       0   \n",
       "2                             0                                       0   \n",
       "3                             0                                       1   \n",
       "4                             0                                       0   \n",
       "\n",
       "   Unconscious bias  Dismissive attitude  Excessive control  Social  Toxic  \\\n",
       "0                 0                    0                  0       0      0   \n",
       "1                 0                    0                  0       1      0   \n",
       "2                 0                    0                  0       1      0   \n",
       "3                 0                    0                  0       0      0   \n",
       "4                 0                    0                  0       1      0   \n",
       "\n",
       "   AntiSocial  NonToxic  \n",
       "0           1         1  \n",
       "1           0         1  \n",
       "2           0         1  \n",
       "3           1         1  \n",
       "4           0         1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fullDataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh2C9zWI0VWZ"
   },
   "source": [
    "**1. Lowercasing:**\n",
    "\n",
    "  Convert all text to lowercase to ensure uniformity and reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1701591979872,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "i4_IKyfwwrfa",
    "outputId": "29e7a790-eee8-434b-ac45-b0fe5fcbddb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    /It may happen that a service die/A service ma...\n",
       "1    @zhiyan, thanks for helping explanation. Overa...\n",
       "2    all the code you have inline below should be r...\n",
       "3    All you do in the interrupt handler is call wa...\n",
       "4    Are you sure this leads to a color that makes ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'] = df['description'].str.lower()\n",
    "df['description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohdK2xgrMXoL"
   },
   "source": [
    "**2. URL removal (URL-rem):**\n",
    "\n",
    " A code review comment may include an URL (e.g., reference to docu- mentation or a StackOverflow post). Although URLs are irrelevant for a antisociality classifier, they can increase the number of features for supervised classifiers. We used a regular expression matcher to identify and remove all URLs from our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1701591986602,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "fDQHBQMbMYD4",
    "outputId": "4a7c4699-a738-446c-d97d-c6dd7a323e81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\('\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/2445626737.py:3: SyntaxWarning: invalid escape sequence '\\('\n",
      "  url_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'> This approach should cover your example (because user will\\n  > get a 404). And that should also cover \"other cases\" (e.g.\\n  > network problem between Glance and the backend store)\\n  > which the user shouldn\\'t know/care about.\\n  \\n  it doesn\\'t 404 was just an example, it can be anything, another example.\\n  \\n  - me: upload image http://gdfgdgf\\n  - glance: NotFound(\"image url failed fetch returned: <Any HTTP status that you can think about>\")\\n  - me: Ohh shit my url is not working because ...\\n  - me: upload image http://<correct URI>\\n  \\n  Like i said if we agree that reason doesn\\'t contain any security issue then why hide it at all, if it\\'s not meant for the user (special case faulty switch) so be it, if it\\'s user problem that a big win.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return url_regex.sub(\" \", text)\n",
    "\n",
    "df['description'] = df['description'].apply(remove_url)\n",
    "df['description'][106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different = df['description_url'] != df['description']\n",
    "different.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRimDsq26IAO"
   },
   "source": [
    "**3. Contraction expansion (Cntr-exp):**\n",
    "\n",
    "  Contractions, which are shortened form of one or two words, are common among code review texts. For example, some common words are: doesn’t →does not, we’re →we are. By creating two different lexicons of the same term, contractions increase the number of unique lexicons and add redundant features. We replaced the commonly used 154 contractions, each with its expanded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1701591992747,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "eFtW6e09BEHA"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                       \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\",\n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                       \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "                       \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "                       \"here's\": \"here is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                       \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                       \"what'll\": \"what will\",\n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\", \"aint\": \"is not\", \"arent\": \"are not\",\n",
    "                       \"cant\": \"cannot\", \"cause\": \"because\",\n",
    "                       \"couldve\": \"could have\", \"couldnt\": \"could not\",\n",
    "                       \"didnt\": \"did not\", \"doesnt\": \"does not\",\n",
    "                       \"dont\": \"do not\", \"hadnt\": \"had not\", \"hasnt\": \"has not\",\n",
    "                       \"havent\": \"have not\", \"howdy\": \"how do you\",\n",
    "                       \"its\": \"it is\", \"lets\": \"let us\", \"maam\": \"madam\", \"maynt\": \"may not\",\n",
    "                       \"mightve\": \"might have\", \"mightnt\": \"might not\",\n",
    "                       \"mightntve\": \"might not have\", \"mustve\": \"must have\",\n",
    "                       \"mustnt\": \"must not\", \"mustntve\": \"must not have\",\n",
    "                       \"neednt\": \"need not\", \"needntve\": \"need not have\",\n",
    "                       \"oclock\": \"of the clock\", \"oughtnt\": \"ought not\",\n",
    "                       \"shouldve\": \"should have\", \"shouldnt\": \"should not\",\n",
    "                       \"werent\": \"were not\", \"yall\": \"you all\", \"youre\": \"you are\",\n",
    "                       \"youve\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1701591998356,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "MJdfXz3eBwBa"
   },
   "outputs": [],
   "source": [
    "def expand_contraction(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\", \"'\"]\n",
    "\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Apply contraction expansion to the specified column\n",
    "df['description'] = df['description'].apply(expand_contraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhOo42bQ0kur"
   },
   "source": [
    "**4. Removing Stopwords:**\n",
    "\n",
    "  Remove common words (stopwords) that don't contribute much to the meaning of the text. Examples include \"and,\" \"the,\" \"is,\" etc. This can be done to reduce noise in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1701592005444,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "xk9qiwzw0sRa",
    "outputId": "dde82908-4d27-4f88-a894-8c1983a4d8b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fatemeh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/fatemeh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Apply stopwords removal to the specified column\n",
    "df['description'] = df['description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MECjNqZp5G4U"
   },
   "source": [
    "**5. Symbol removal (Sym-rem):**\n",
    "\n",
    " Since special symbols (e.g., &, #, and ˆ ) are irrelevant for antisociality classification tasks, we use a regular expression matcher to identify and remove special symbols.\n",
    "\n",
    " This code uses the re module to define a regular expression pattern (r'[^\\w\\s]') that matches any character that is not a word character or whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1701592009636,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "JaOJGCFy5Pqp"
   },
   "outputs": [],
   "source": [
    "# Function to remove special symbols using regular expression\n",
    "def remove_special_symbols(text):\n",
    "    # Define a regular expression pattern to match special symbols\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    # Use re.sub to replace matched symbols with an empty string\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Apply symbol removal to the specified column\n",
    "df['description'] = df['description'].apply(remove_special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1701592012957,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "dUDrcxykbjv6",
    "outputId": "f47dd5a5-5c11-44e1-a745-89d732466449"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' approach cover example  user  get 404   also cover  cases   eg   network problem glance backend store   user knowcare  404 example  anything  another example    upload image  glance  notfound   image url failed fetch returned   http status think      ohh shit url working    upload image uri  like said agree reason contain security issue hide  meant user  special case faulty switch   user problem big win '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'][106]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-NF5aZukcLB"
   },
   "source": [
    "**6. Repetition elimination (Rep-elm):**\n",
    "\n",
    " A person may repeat some of the characters to misspell an antisocial word to evade detection from a dictionary based antisociality detectors. For example, in the sentence “You’re duumbbbb!”, ‘dumb’ is misspelled through character repetitions. We have created a pattern based matcher to identify such misspelled cases and replace each with its correctly spelled form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1701592017817,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "JXTlrqSrkXxZ"
   },
   "outputs": [],
   "source": [
    "# Function to eliminate repetitions using regular expression\n",
    "def eliminate_repetitions(text):\n",
    "    # Define a regular expression pattern to match repeated characters (at least two repetitions)\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    # Use re.sub to replace repeated characters with a single occurrence\n",
    "    text = pattern.sub(r'\\1', text)\n",
    "    return text\n",
    "\n",
    " # Apply repetition elimination to the specified column\n",
    "df['description'] = df['description'].apply(eliminate_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS5Pm8uX5QJL"
   },
   "source": [
    " **7. Adversarial pattern identification (Adv-ptrn):**\n",
    "\n",
    " A person may misspell profane words by replacing some characters with a symbol (e.g., ‘f*ck’ and ‘b!tch’) or use an acronym for a slang (e.g., ‘stfu’).\n",
    "\n",
    " To identify such cases, we have developed a profanity preprocessor, which includes pattern matchers to identify various forms of the 85 commonly used profane words. Our preprocessor replaces each identified case with its correctly spelled form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1701592021317,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "RXvHABQ95rET"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:180: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:185: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:180: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:185: SyntaxWarning: invalid escape sequence '\\*'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:6: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:8: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'feck ', ' fux ', 'f\\*\\*',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:9: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:9: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:15: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  ' c[!@#\\$%\\^\\&\\*]*r[!@#\\$%\\^&\\*]*p', 'cr@p', ' c r a p',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:20: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:21: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:22: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:27: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  ' bi\\+ch', ' b!\\+ch', ' (b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:33: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  ' bi\\+ch', ' b!\\+ch', ' (b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:34: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  ' biatch', ' bi\\*\\*h', ' bytch', 'b i t c h'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:75: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'bullsh\\*t', 'bull\\$hit', 'bull sh.t'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:75: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  'bullsh\\*t', 'bull\\$hit', 'bull sh.t'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:95: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', 'sh\\*tty',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:95: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', 'sh\\*tty',\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'sh\\*ty', 'sh\\*t'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:96: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'sh\\*ty', 'sh\\*t'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:180: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'wh\\*\\*\\*', 'w h o r e'\n",
      "/var/folders/wv/p256rvnn78bd_8sv0_20lz4m0000gn/T/ipykernel_99336/3214880584.py:185: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  'ha\\*\\*\\*ha',\n"
     ]
    }
   ],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' fuck ':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "    ' crap ':\n",
    "        [\n",
    "            ' (c)(r|[^a-z0-9 ])(a|[^a-z0-9 ])(p|[^a-z0-9 ])([^ ])*',\n",
    "            ' (c)([^a-z]*)(r)([^a-z]*)(a)([^a-z]*)(p)',\n",
    "            ' c[!@#\\$%\\^\\&\\*]*r[!@#\\$%\\^&\\*]*p', 'cr@p', ' c r a p',\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'bitches', ' b[w]*i[t]*ch', ' b!tch',\n",
    "            ' bi\\+ch', ' b!\\+ch', ' (b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            ' biatch', ' bi\\*\\*h', ' bytch', 'b i t c h'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay', 'homo'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'd i c k'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit', 'bull sh.t'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots' 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', 'sh\\*tty',\n",
    "            'sh\\*ty', 'sh\\*t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole', 'sh.thole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            'raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            ' stfu' '^stfu'\n",
    "        ],\n",
    "\n",
    "    ' for your fucking information':\n",
    "        [\n",
    "            ' fyfi', '^fyfi'\n",
    "        ],\n",
    "    ' get the fuck off':\n",
    "        [\n",
    "            'gtfo', '^gtfo'\n",
    "        ],\n",
    "\n",
    "    ' oh my fucking god ':\n",
    "        [\n",
    "            ' omfg', '^omfg'\n",
    "        ],\n",
    "\n",
    "    ' what the hell ':\n",
    "        [\n",
    "            ' wth', '^wth'\n",
    "        ],\n",
    "\n",
    "    ' what the fuck ':\n",
    "        [\n",
    "            ' wtf', '^wtf'\n",
    "        ],\n",
    "    ' son of bitch ':\n",
    "        [\n",
    "            ' sob ', '^sob '\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', '(p)(u|[^a-z0-9 ])(s|[^a-z0-9 ])(s|[^a-z0-9 ])(y)',\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "\n",
    "    ' haha ':\n",
    "        [\n",
    "            'ha\\*\\*\\*ha',\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1701592033143,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "aWRrPswDhIeJ"
   },
   "outputs": [],
   "source": [
    "# Function to eliminate repetitions based on patterns\n",
    "def profanity_detector(text, patterns):\n",
    "    for word, word_patterns in patterns.items():\n",
    "        for pattern in word_patterns:\n",
    "            # Use re.sub to replace repeated patterns with the correct spelling\n",
    "            text = re.sub(pattern, f' {word} ', text)\n",
    "    return text\n",
    "\n",
    "# Apply repetition elimination to the specified column\n",
    "df['description'] = df['description'].apply(lambda x: profanity_detector(x, RE_PATTERNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(207)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different = df['description_programming_keywords_remove'] != df['description']\n",
    "different.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6LvRDPw7R5j"
   },
   "source": [
    "**8. Identifier splitting (Id-split):**\n",
    "\n",
    " In this preprocessing, we use a regular expression matcher to split identifiers written in both camelCase and under_score forms. For example, this step will replace ‘isCrap’ with ‘is Crap’ and replace ‘is_shitty’ with ‘is shitty’. This preprocessing may help to identify example code segments with profane words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1701592035872,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "GoBEDuUF7YtS"
   },
   "outputs": [],
   "source": [
    "# Function to split identifiers based on patterns\n",
    "def split_identifiers(text):\n",
    "      result = re.sub('[_]+', ' ', text) # replace underscores with space\n",
    "      result=re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', result))\n",
    "      return result\n",
    "\n",
    "# Apply repetition elimination to the specified column\n",
    "df['description'] = df['description'].apply(split_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ppZhZzI7ZDF"
   },
   "source": [
    "**9. Programming Keywords Removal (Kwrd-rem):**\n",
    "\n",
    " Code review texts often include programming language specific keywords (e.g., ‘while’, ‘case’, ‘if’, ‘catch’, and ‘except’). These keywords are SE domain specific jargon and are not useful for toxicity prediction. We have created a list of 90 programming keywords used in the popular programming languages (e.g., C++, Java, Python, C#, PHP, JavaScript, and Go). This step searches and removes occurrences of those programming keywords from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1701592040044,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "JSuVt1h3uvhS",
    "outputId": "f3806288-3365-4950-9896-52173d30b003"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['while',\n",
       " 'case',\n",
       " 'switch',\n",
       " 'def',\n",
       " 'abstract',\n",
       " 'byte',\n",
       " 'continue',\n",
       " 'native',\n",
       " 'private',\n",
       " 'synchronized',\n",
       " 'if',\n",
       " 'do',\n",
       " 'include',\n",
       " 'each',\n",
       " 'than',\n",
       " 'finally',\n",
       " 'class',\n",
       " 'double',\n",
       " 'float',\n",
       " 'int',\n",
       " 'else',\n",
       " 'instanceof',\n",
       " 'long',\n",
       " 'super',\n",
       " 'import',\n",
       " 'short',\n",
       " 'default',\n",
       " 'catch',\n",
       " 'try',\n",
       " 'new',\n",
       " 'final',\n",
       " 'extends',\n",
       " 'implements',\n",
       " 'public',\n",
       " 'protected',\n",
       " 'static',\n",
       " 'this',\n",
       " 'return',\n",
       " 'char',\n",
       " 'const',\n",
       " 'break',\n",
       " 'boolean',\n",
       " 'bool',\n",
       " 'package',\n",
       " 'byte',\n",
       " 'assert',\n",
       " 'raise',\n",
       " 'global',\n",
       " 'with',\n",
       " 'or',\n",
       " 'yield',\n",
       " 'in',\n",
       " 'out',\n",
       " 'except',\n",
       " 'and',\n",
       " 'enum',\n",
       " 'signed',\n",
       " 'void',\n",
       " 'virtual',\n",
       " 'union',\n",
       " 'goto',\n",
       " 'var',\n",
       " 'function',\n",
       " 'require',\n",
       " 'print',\n",
       " 'echo',\n",
       " 'foreach',\n",
       " 'elseif',\n",
       " 'namespace',\n",
       " 'delegate',\n",
       " 'event',\n",
       " 'override',\n",
       " 'struct',\n",
       " 'readonly',\n",
       " 'explicit',\n",
       " 'interface',\n",
       " 'get',\n",
       " 'set',\n",
       " 'elif',\n",
       " 'for',\n",
       " 'throw',\n",
       " 'throws',\n",
       " 'lambda',\n",
       " 'endfor',\n",
       " 'endforeach',\n",
       " 'endif',\n",
       " 'endwhile',\n",
       " 'clone',\n",
       " 'ifdef',\n",
       " 'mk']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programming_keywords=[]\n",
    "# Open the file in read mode\n",
    "with open('programming_keywords.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    for line in file:\n",
    "        programming_keywords.append(line.strip())\n",
    "\n",
    "programming_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1701592044057,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "remCVoaB7zHj"
   },
   "outputs": [],
   "source": [
    "def remove_keywords(text):\n",
    "        words = text.split()\n",
    "        resultwords = [word for word in words if word.lower() not in programming_keywords]\n",
    "        result = ' '.join(resultwords)\n",
    "        return result\n",
    "\n",
    "# Apply remove keywords to the specified column\n",
    "df['description'] = df['description'].apply(remove_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1701592047035,
     "user": {
      "displayName": "Marzieh Sadri",
      "userId": "16629577950148539714"
     },
     "user_tz": -210
    },
    "id": "KJVHGVsKxU_D"
   },
   "outputs": [],
   "source": [
    "# Specify the path for the CSV file\n",
    "csv_file_path = '/content/drive/MyDrive/codeReview/3_preprocessing/preprocessedData.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Statistics Before vs. After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_csv('preprocessedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "def vocabulary_size(texts):\n",
    "    vocab = set()\n",
    "    for t in texts:\n",
    "        vocab.update(tokenize(t))\n",
    "    return len(vocab)\n",
    "\n",
    "def token_stats(texts):\n",
    "    lengths = [len(tokenize(t)) for t in texts]\n",
    "    return {\n",
    "        \"avg_tokens\": sum(lengths) / len(lengths),\n",
    "        \"median_tokens\": sorted(lengths)[len(lengths)//2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = vocabulary_size(df_preprocessed['description'])\n",
    "befor = vocabulary_size(df['description'])\n",
    "100 - ((after/befor)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens': 25.495016611295682, 'median_tokens': 16}\n",
      "{'avg_tokens': 15.20265780730897, 'median_tokens': 8}\n"
     ]
    }
   ],
   "source": [
    "print(token_stats(df['description']))\n",
    "print (token_stats(df_preprocessed['description']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObVAjmg3h22mk/pSJo6zVw",
   "mount_file_id": "12DhG-7twoMdB1dGKuSd95KmfYBk4eh-7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hfagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
