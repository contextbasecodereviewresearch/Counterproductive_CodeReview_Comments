{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21263,"status":"ok","timestamp":1702733659223,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"XejeChBSzFrG","outputId":"ebb603ad-e892-49ce-aa4b-2f9ab89016bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":2543,"status":"ok","timestamp":1702733666600,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"SFjimJHGznUH","outputId":"0d22f371-a13a-4ccf-c628-5265ceffe782"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    description  Mockery\n","0             must black belt copypasting bravo        1\n","1    invent programing language understand word        1\n","2     fan obfuscation code masterclas confusion        1\n","3               mispel bug feature comit mesage        1\n","4  se fan what the fuck per minute coding style        1"],"text/html":["\n","  <div id=\"df-9da538f8-d8d4-43d4-89ae-ff08a628cb1c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>description</th>\n","      <th>Mockery</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>must black belt copypasting bravo</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>invent programing language understand word</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>fan obfuscation code masterclas confusion</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>mispel bug feature comit mesage</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>se fan what the fuck per minute coding style</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9da538f8-d8d4-43d4-89ae-ff08a628cb1c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9da538f8-d8d4-43d4-89ae-ff08a628cb1c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9da538f8-d8d4-43d4-89ae-ff08a628cb1c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b5955a55-0a07-4445-986c-fcdc1f869eaa\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5955a55-0a07-4445-986c-fcdc1f869eaa')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b5955a55-0a07-4445-986c-fcdc1f869eaa button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/codeReview/5_models/Mockery/preprocessed.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"WrR4y8bcAFy0"},"source":["**define function for save embedding representation**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1702733681962,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"mQ8M2aXJAPyn"},"outputs":[],"source":["def save_representation(df, file_path):\n","  # Write the DataFrame to a CSV file\n","  df.to_csv(file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"DCx8v2XNuHl-"},"source":["Vectorizing code review comments involves converting textual data into numerical vectors so that machine learning algorithms can process and analyze them. Here are several common approaches that we implement on our data to convert them from text to nimeric.\n","\n","**1. Bag-of-Words (BoW):**\n","\n"," BoW represents a document as an unordered set of words, disregarding grammar and word order but considering word frequency. we use  the CountVectorizer from scikit-learn to implement BoW."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bepl1SmxfidB"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","def create_bow(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the Bag-of-Words representation.\n","    \"\"\"\n","    # Create an instance of CountVectorizer\n","    vectorizer = CountVectorizer()\n","\n","    # Fit and transform the comments to obtain the Bag-of-Words matrix\n","    bow_matrix = vectorizer.fit_transform(comments)\n","\n","    # Convert the Bag-of-Words matrix to a DataFrame\n","    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","    return bow_df\n","\n","# Create Bag-of-Words representation for the 'comments' column\n","bow_representation = create_bow(df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKtfKXDBju_w"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/bow_representation.csv'\n","save_representation(bow_representation,csv_file_path )"]},{"cell_type":"markdown","metadata":{"id":"x76shBoMuxNx"},"source":["**2. Term Frequency-Inverse Document Frequency (TF-IDF):**\n","\n","Similar to BoW, but it also considers the importance of words by giving higher weights to terms that are rare across all documents.\n","The TfidfVectorizer from scikit-learn is commonly used for TF-IDF vectorization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnwuUQSXu_up"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def create_tfidf(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the TF-IDF representation.\n","    \"\"\"\n","\n","    # Create an instance of TfidfVectorizer\n","    vectorizer = TfidfVectorizer()\n","\n","    # Fit and transform the comments to obtain the TF-IDF matrix\n","    tfidf_matrix = vectorizer.fit_transform(comments)\n","\n","    # Convert the TF-IDF matrix to a DataFrame\n","    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","    return tfidf_df\n","\n","# Create TF-IDF representation for the 'comments' column\n","tfidf_representation = create_tfidf(df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24pnFv_pwNEt"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/tfidf_representation.csv'\n","\n","# Save TF-IDF representation to a CSV file\n","save_representation(tfidf_representation,csv_file_path )"]},{"cell_type":"markdown","metadata":{"id":"MF848EnSvA-C"},"source":["**Word Embeddings (Word2Vec, GloVe, FastText):**\n","\n","Word embeddings capture semantic relationships between words by representing them as dense vectors in a continuous vector space.\n","Gensim provides implementations for Word2Vec, and we can find pre-trained models for GloVe and FastText.\n","\n","\n","\n","**3. Word2Vec**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1332,"status":"ok","timestamp":1702126830927,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"6AwW5y2upE0v","outputId":"7209b49f-fb78-434c-cdce-3a2f387394bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","# Tokenize the comments\n","tokenized_comments = df['description'].apply(lambda x: word_tokenize(x.lower()))  # Assuming comments are preprocessed and lowercased\n","\n","# Train the Word2Vec model\n","model = Word2Vec(sentences=tokenized_comments, vector_size=100, window=5, min_count=1, workers=4)\n","\n","def get_comment_vector(comment):\n","    tokens = word_tokenize(comment.lower())\n","    vector = sum(model.wv[word] for word in tokens if word in model.wv) / len(tokens)\n","    return vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRKfsaNdqD2T"},"outputs":[],"source":["# Apply the function to the entire 'comments' column\n","df['comment_vectors'] = df['description'].apply(get_comment_vector)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1702128196829,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"BTfgknFfzMcK","outputId":"2cbbf8e3-67b3-4ccd-9ca1-0ca1dcc2137b"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-8beae1beff7c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Specify the path for the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/codeReview/4_wordVectorization/word2vec_representation.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector_dim_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save Word2Vec representation to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"]}],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/word2vec_representation.csv'\n","df1[['vector_dim_' + str(i) for i in range(df['comment_vectors'].iloc[0].shape[0])]] = pd.DataFrame(df['comment_vectors'].tolist(), index=df.index)\n","\n","# Save Word2Vec representation to a CSV file\n","df1.to_csv(csv_file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"JU9hVXTjvkGB"},"source":["**4. GloVe**\n","\n","It stands for Global Vectors. This is created by Stanford University. Glove has pre-defined dense vectors for around every 6 billion words of English literature along with many other general use characters like comma, braces, and semicolons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6airamBXvnwJ"},"outputs":[],"source":["import spacy\n","\n","# Load spaCy model with GloVe embeddings\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","def create_glove(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the GloVe representation.\n","    \"\"\"\n","    # Process comments with spaCy to get GloVe vectors\n","    glove_vectors = [nlp(comment).vector for comment in comments]\n","\n","    # Convert the GloVe vectors to a DataFrame\n","    glove_df = pd.DataFrame(glove_vectors)\n","\n","    return glove_df\n","\n","\n","# Create GloVe representation for the 'comments' column\n","glove_representation = create_glove(df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VbHVAm61I_j"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/glove_representation.csv'\n","\n","# Save GloVe representation to a CSV file\n","save_representation(glove_representation, csv_file_path)"]},{"cell_type":"markdown","metadata":{"id":"IV_wvemzvoDG"},"source":["**5. FastText**\n","\n","fastText is an open-source library, developed by the Facebook AI Research lab. Its main focus is on achieving scalable solutions for the tasks of text classification and representation while processing large datasets quickly and accurately. FastText is a modified version of word2vec."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7xYwNwJrvrBB","executionInfo":{"status":"ok","timestamp":1702733765991,"user_tz":-210,"elapsed":3237,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["from gensim.models import FastText\n","\n","def create_fasttext_embedding_model(comments):\n","\n","    # Tokenize comments into sentences\n","    sentences = [comment.split() for comment in comments]\n","\n","    # Train FastText model\n","    model = FastText(sentences, vector_size=128, window=5, min_count=3, workers=4)\n","\n","    return model\n","\n","# Create FastText representation for the 'comments' column\n","fasttext_representation = create_fasttext_embedding_model(df['description'])\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"o0tld0aIC03T","executionInfo":{"status":"ok","timestamp":1702733781609,"user_tz":-210,"elapsed":11542,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/5_models/Mockery/fasttext_representation.bin'\n","\n","# Save the trained FastText model\n","fasttext_representation.save(csv_file_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pVQyuJZHQyCP","executionInfo":{"status":"ok","timestamp":1702733793720,"user_tz":-210,"elapsed":4006,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["def create_fasttext(df, column_name, model, output_csv):\n","    \"\"\"\n","    Save FastText embedding vectors for each comment in a pandas DataFrame to a CSV file.\n","\n","    Parameters:\n","    - df: pandas DataFrame\n","        The DataFrame containing the comments.\n","    - column_name: str\n","        The name of the column containing the comments.\n","    - model: gensim.models.fasttext.FastText\n","        Trained FastText model.\n","    - output_csv: str\n","        Path to the output CSV file.\n","    \"\"\"\n","\n","    # Extract comments from the specified column\n","    comments = df[column_name]\n","\n","    # Tokenize comments into sentences\n","    sentences = [comment.split() for comment in comments]\n","\n","    # Get FastText embeddings for each comment\n","    embeddings = [model.wv[words].mean(axis=0) for words in sentences]\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings, columns=[f'feature_{i}' for i in range(model.vector_size)])\n","\n","    # Save the result to a CSV file\n","    vectors_df.to_csv(output_csv, index=False)\n","\n","\n","\n","# Load the saved FastText model\n","loaded_fasttext_model = FastText.load(\"/content/drive/MyDrive/codeReview/5_models/Mockery/fasttext_representation.bin\")\n","\n","# Specify the path to the output CSV file\n","output_csv_path = \"/content/drive/MyDrive/codeReview/5_models/Mockery/fasttext_representation.csv\"\n","\n","# Save FastText embeddings for each comment to a CSV file\n","create_fasttext(df, 'description', loaded_fasttext_model, output_csv_path)"]},{"cell_type":"markdown","metadata":{"id":"Qg8P4f0YwEcq"},"source":["**6. Universal Sentence Encoder (USE):**\n","\n","Developed by Google, USE generates fixed-size vectors for input sentences. It captures semantic information and can be useful for various natural language processing tasks.\n","TensorFlow provides a pre-trained Universal Sentence Encoder."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"At92JtGHwJyh","executionInfo":{"status":"ok","timestamp":1702733806016,"user_tz":-210,"elapsed":6198,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","def create_use(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the Universal Sentence Encoder embeddings.\n","    \"\"\"\n","    # Load the Universal Sentence Encoder module\n","    use_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n","    embed = hub.load(use_url)\n","\n","    # Get embeddings for each comment\n","    embeddings = embed(comments)\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings.numpy(), columns=[f'feature_{i}' for i in range(embeddings.shape[1])])\n","\n","    return vectors_df"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"GR6-rZtdV89T","executionInfo":{"status":"ok","timestamp":1702733838132,"user_tz":-210,"elapsed":27926,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["# Create USE representation for the 'comments' column\n","use_representation = create_use(df['description'])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"yftGtS59V1ad","executionInfo":{"status":"ok","timestamp":1702733841490,"user_tz":-210,"elapsed":676,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"}}},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/5_models/Mockery/use_representation.csv'\n","\n","# Save GloVe representation to a CSV file\n","use_representation.to_csv(csv_file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"VokPHxGcwM8J"},"source":["**7. BERT Embeddings:**\n","\n","BERT (Bidirectional Encoder Representations from Transformers) provides context-aware word embeddings, capturing the meaning of words in the context of the entire sentence.\n","The transformers library in Python provides access to pre-trained BERT models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XF7XvtFMxE6S"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","def create_bert(df, column_name, model_name=\"bert-base-uncased\"):\n","    \"\"\"re-trained BERT model.\n","\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the BERT embeddings.\n","    \"\"\"\n","\n","    # Load BERT tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name)\n","\n","    # Extract comments from the specified column\n","    comments = df[column_name].tolist()\n","\n","    # Tokenize and encode comments\n","    encoded_comments = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","    # Forward pass to get BERT embeddings\n","    with torch.no_grad():\n","        outputs = model(**encoded_comments)\n","\n","    # Extract the embeddings from the last layer\n","    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings, columns=[f'feature_{i}' for i in range(embeddings.shape[1])])\n","\n","    return vectors_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7rmhZl3fG08"},"outputs":[],"source":["# Create BERT representation for the 'comments' column\n","bert_representation = create_bert(df, 'description')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPPlGjLrfHIw"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/bert_representation.csv'\n","\n","# Save BERT representation to a CSV file\n","bert_representation.to_csv(csv_file_path, index=False)"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"19DMbzXxVrYL3dLWHIfV5yaeS6lV9iz5k","authorship_tag":"ABX9TyPhdhCKMif27GQBdLgiXGV+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}