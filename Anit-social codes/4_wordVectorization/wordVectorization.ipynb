{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34436,"status":"ok","timestamp":1702122702594,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"XejeChBSzFrG","outputId":"ecb8886e-6467-4c6d-adae-46b96f7780fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1702122718671,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"SFjimJHGznUH","outputId":"306f9ebd-76fc-4021-dc1f-6de79aa944be"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-dfbf84c9-5484-4906-8ccf-8b2c93048573\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003edescription\u003c/th\u003e\n","      \u003cth\u003ePersonal attacks\u003c/th\u003e\n","      \u003cth\u003eThreats or intimidation\u003c/th\u003e\n","      \u003cth\u003eMockery\u003c/th\u003e\n","      \u003cth\u003eLack of specificity\u003c/th\u003e\n","      \u003cth\u003eDiscouragement without guide\u003c/th\u003e\n","      \u003cth\u003eDisregard for other time or boundaries\u003c/th\u003e\n","      \u003cth\u003eUnconscious bias\u003c/th\u003e\n","      \u003cth\u003eDismissive attitude\u003c/th\u003e\n","      \u003cth\u003eExcessive control\u003c/th\u003e\n","      \u003cth\u003eSocial\u003c/th\u003e\n","      \u003cth\u003eToxic\u003c/th\u003e\n","      \u003cth\u003eAntiSocial\u003c/th\u003e\n","      \u003cth\u003eNonToxic\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eit may hapen service diea service may die\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003ezhiyan thanks helping explanation overal corec...\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003ecode inline refactored put main main main\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003einterupt handler cal wake up loks like ned thr...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003esure leads color makes white leter readable ne...\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfbf84c9-5484-4906-8ccf-8b2c93048573')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-dfbf84c9-5484-4906-8ccf-8b2c93048573 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dfbf84c9-5484-4906-8ccf-8b2c93048573');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-e6618135-34ea-4f1c-af21-570a70d1b109\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6618135-34ea-4f1c-af21-570a70d1b109')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e6618135-34ea-4f1c-af21-570a70d1b109 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["                                         description  Personal attacks  \\\n","0          it may hapen service diea service may die                 0   \n","1  zhiyan thanks helping explanation overal corec...                 0   \n","2          code inline refactored put main main main                 0   \n","3  interupt handler cal wake up loks like ned thr...                 1   \n","4  sure leads color makes white leter readable ne...                 0   \n","\n","   Threats or intimidation  Mockery  Lack of specificity  \\\n","0                        0        0                    1   \n","1                        0        0                    0   \n","2                        0        0                    0   \n","3                        0        1                    1   \n","4                        0        0                    0   \n","\n","   Discouragement without guide  Disregard for other time or boundaries  \\\n","0                             0                                       0   \n","1                             0                                       0   \n","2                             0                                       0   \n","3                             0                                       1   \n","4                             0                                       0   \n","\n","   Unconscious bias  Dismissive attitude  Excessive control  Social  Toxic  \\\n","0                 0                    0                  0       0      0   \n","1                 0                    0                  0       1      0   \n","2                 0                    0                  0       1      0   \n","3                 0                    0                  0       0      0   \n","4                 0                    0                  0       1      0   \n","\n","   AntiSocial  NonToxic  \n","0           1         1  \n","1           0         1  \n","2           0         1  \n","3           1         1  \n","4           0         1  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/codeReview/3_preprocessing/preprocessedData.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"WrR4y8bcAFy0"},"source":["**define function for save embedding representation**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1702123126133,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"mQ8M2aXJAPyn"},"outputs":[],"source":["def save_representation(df, file_path):\n","  # Write the DataFrame to a CSV file\n","  df.to_csv(file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"DCx8v2XNuHl-"},"source":["Vectorizing code review comments involves converting textual data into numerical vectors so that machine learning algorithms can process and analyze them. Here are several common approaches that we implement on our data to convert them from text to nimeric.\n","\n","**1. Bag-of-Words (BoW):**\n","\n"," BoW represents a document as an unordered set of words, disregarding grammar and word order but considering word frequency. we use  the CountVectorizer from scikit-learn to implement BoW."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":806,"status":"ok","timestamp":1702123134882,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"bepl1SmxfidB"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","def create_bow(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the Bag-of-Words representation.\n","    \"\"\"\n","    # Create an instance of CountVectorizer\n","    vectorizer = CountVectorizer()\n","\n","    # Fit and transform the comments to obtain the Bag-of-Words matrix\n","    bow_matrix = vectorizer.fit_transform(comments)\n","\n","    # Convert the Bag-of-Words matrix to a DataFrame\n","    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","    return bow_df\n","\n","# Create Bag-of-Words representation for the 'comments' column\n","bow_representation = create_bow(df['description'])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":868,"status":"ok","timestamp":1702123140936,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"OKtfKXDBju_w"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/bow_representation.csv'\n","save_representation(bow_representation,csv_file_path )"]},{"cell_type":"markdown","metadata":{"id":"x76shBoMuxNx"},"source":["**2. Term Frequency-Inverse Document Frequency (TF-IDF):**\n","\n","Similar to BoW, but it also considers the importance of words by giving higher weights to terms that are rare across all documents.\n","The TfidfVectorizer from scikit-learn is commonly used for TF-IDF vectorization."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1702124126694,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"bnwuUQSXu_up"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def create_tfidf(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the TF-IDF representation.\n","    \"\"\"\n","\n","    # Create an instance of TfidfVectorizer\n","    vectorizer = TfidfVectorizer()\n","\n","    # Fit and transform the comments to obtain the TF-IDF matrix\n","    tfidf_matrix = vectorizer.fit_transform(comments)\n","\n","    # Convert the TF-IDF matrix to a DataFrame\n","    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","    return tfidf_df\n","\n","# Create TF-IDF representation for the 'comments' column\n","tfidf_representation = create_tfidf(df['description'])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1755,"status":"ok","timestamp":1702124130751,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"24pnFv_pwNEt"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/tfidf_representation.csv'\n","\n","# Save TF-IDF representation to a CSV file\n","save_representation(tfidf_representation,csv_file_path )"]},{"cell_type":"markdown","metadata":{"id":"MF848EnSvA-C"},"source":["**Word Embeddings (Word2Vec, GloVe, FastText):**\n","\n","Word embeddings capture semantic relationships between words by representing them as dense vectors in a continuous vector space.\n","Gensim provides implementations for Word2Vec, and we can find pre-trained models for GloVe and FastText.\n","\n","\n","\n","**3. Word2Vec**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1332,"status":"ok","timestamp":1702126830927,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"6AwW5y2upE0v","outputId":"7209b49f-fb78-434c-cdce-3a2f387394bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","# Tokenize the comments\n","tokenized_comments = df['description'].apply(lambda x: word_tokenize(x.lower()))  # Assuming comments are preprocessed and lowercased\n","\n","# Train the Word2Vec model\n","model = Word2Vec(sentences=tokenized_comments, vector_size=100, window=5, min_count=1, workers=4)\n","\n","def get_comment_vector(comment):\n","    tokens = word_tokenize(comment.lower())\n","    vector = sum(model.wv[word] for word in tokens if word in model.wv) / len(tokens)\n","    return vector"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1702127894153,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"NRKfsaNdqD2T"},"outputs":[],"source":["# Apply the function to the entire 'comments' column\n","df['comment_vectors'] = df['description'].apply(get_comment_vector)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1702128196829,"user":{"displayName":"Marzieh Sadri","userId":"16629577950148539714"},"user_tz":-210},"id":"BTfgknFfzMcK"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-24-8beae1beff7c\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Specify the path for the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/codeReview/4_wordVectorization/word2vec_representation.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector_dim_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save Word2Vec representation to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"]}],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/word2vec_representation.csv'\n","df1[['vector_dim_' + str(i) for i in range(df['comment_vectors'].iloc[0].shape[0])]] = pd.DataFrame(df['comment_vectors'].tolist(), index=df.index)\n","\n","# Save Word2Vec representation to a CSV file\n","df1.to_csv(csv_file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"JU9hVXTjvkGB"},"source":["**4. GloVe**\n","\n","It stands for Global Vectors. This is created by Stanford University. Glove has pre-defined dense vectors for around every 6 billion words of English literature along with many other general use characters like comma, braces, and semicolons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6airamBXvnwJ"},"outputs":[],"source":["import spacy\n","\n","# Load spaCy model with GloVe embeddings\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","def create_glove(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the GloVe representation.\n","    \"\"\"\n","    # Process comments with spaCy to get GloVe vectors\n","    glove_vectors = [nlp(comment).vector for comment in comments]\n","\n","    # Convert the GloVe vectors to a DataFrame\n","    glove_df = pd.DataFrame(glove_vectors)\n","\n","    return glove_df\n","\n","\n","# Create GloVe representation for the 'comments' column\n","glove_representation = create_glove(df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VbHVAm61I_j"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/glove_representation.csv'\n","\n","# Save GloVe representation to a CSV file\n","save_representation(glove_representation, csv_file_path)"]},{"cell_type":"markdown","metadata":{"id":"IV_wvemzvoDG"},"source":["**5. FastText**\n","\n","fastText is an open-source library, developed by the Facebook AI Research lab. Its main focus is on achieving scalable solutions for the tasks of text classification and representation while processing large datasets quickly and accurately. FastText is a modified version of word2vec."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xYwNwJrvrBB"},"outputs":[],"source":["from gensim.models import FastText\n","\n","def create_fasttext_embedding_model(comments):\n","\n","    # Tokenize comments into sentences\n","    sentences = [comment.split() for comment in comments]\n","\n","    # Train FastText model\n","    model = FastText(sentences, vector_size=128, window=5, min_count=3, workers=4)\n","\n","    return model\n","\n","# Create FastText representation for the 'comments' column\n","fasttext_representation = create_fasttext_embedding_model(df['description'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0tld0aIC03T"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/fasttext_representation.bin'\n","\n","# Save the trained FastText model\n","fasttext_representation.save(csv_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVQyuJZHQyCP"},"outputs":[],"source":["def create_fasttext(df, column_name, model, output_csv):\n","    \"\"\"\n","    Save FastText embedding vectors for each comment in a pandas DataFrame to a CSV file.\n","\n","    Parameters:\n","    - df: pandas DataFrame\n","        The DataFrame containing the comments.\n","    - column_name: str\n","        The name of the column containing the comments.\n","    - model: gensim.models.fasttext.FastText\n","        Trained FastText model.\n","    - output_csv: str\n","        Path to the output CSV file.\n","    \"\"\"\n","\n","    # Extract comments from the specified column\n","    comments = df[column_name]\n","\n","    # Tokenize comments into sentences\n","    sentences = [comment.split() for comment in comments]\n","\n","    # Get FastText embeddings for each comment\n","    embeddings = [model.wv[words].mean(axis=0) for words in sentences]\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings, columns=[f'feature_{i}' for i in range(model.vector_size)])\n","\n","    # Save the result to a CSV file\n","    vectors_df.to_csv(output_csv, index=False)\n","\n","\n","\n","# Load the saved FastText model\n","loaded_fasttext_model = FastText.load(\"/content/drive/MyDrive/codeReview/4_wordVectorization/fasttext_representation.bin\")\n","\n","# Specify the path to the output CSV file\n","output_csv_path = \"/content/drive/MyDrive/codeReview/4_wordVectorization/fasttext_representation.csv\"\n","\n","# Save FastText embeddings for each comment to a CSV file\n","create_fasttext(df, 'description', loaded_fasttext_model, output_csv_path)"]},{"cell_type":"markdown","metadata":{"id":"Qg8P4f0YwEcq"},"source":["**6. Universal Sentence Encoder (USE):**\n","\n","Developed by Google, USE generates fixed-size vectors for input sentences. It captures semantic information and can be useful for various natural language processing tasks.\n","TensorFlow provides a pre-trained Universal Sentence Encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"At92JtGHwJyh"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","def create_use(comments):\n","    \"\"\"\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the Universal Sentence Encoder embeddings.\n","    \"\"\"\n","    # Load the Universal Sentence Encoder module\n","    use_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n","    embed = hub.load(use_url)\n","\n","    # Get embeddings for each comment\n","    embeddings = embed(comments)\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings.numpy(), columns=[f'feature_{i}' for i in range(embeddings.shape[1])])\n","\n","    return vectors_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GR6-rZtdV89T"},"outputs":[],"source":["# Create USE representation for the 'comments' column\n","use_representation = create_use(df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yftGtS59V1ad"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/use_representation.csv'\n","\n","# Save GloVe representation to a CSV file\n","use_representation.to_csv(csv_file_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"VokPHxGcwM8J"},"source":["**7. BERT Embeddings:**\n","\n","BERT (Bidirectional Encoder Representations from Transformers) provides context-aware word embeddings, capturing the meaning of words in the context of the entire sentence.\n","The transformers library in Python provides access to pre-trained BERT models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XF7XvtFMxE6S"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","def create_bert(df, column_name, model_name=\"bert-base-uncased\"):\n","    \"\"\"re-trained BERT model.\n","\n","    Returns:\n","    - pd.DataFrame\n","        A new DataFrame with the BERT embeddings.\n","    \"\"\"\n","\n","    # Load BERT tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name)\n","\n","    # Extract comments from the specified column\n","    comments = df[column_name].tolist()\n","\n","    # Tokenize and encode comments\n","    encoded_comments = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","    # Forward pass to get BERT embeddings\n","    with torch.no_grad():\n","        outputs = model(**encoded_comments)\n","\n","    # Extract the embeddings from the last layer\n","    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n","\n","    # Create a DataFrame with comment vectors\n","    vectors_df = pd.DataFrame(embeddings, columns=[f'feature_{i}' for i in range(embeddings.shape[1])])\n","\n","    return vectors_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7rmhZl3fG08"},"outputs":[],"source":["# Create BERT representation for the 'comments' column\n","bert_representation = create_bert(df, 'description')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPPlGjLrfHIw"},"outputs":[],"source":["# Specify the path for the CSV file\n","csv_file_path = '/content/drive/MyDrive/codeReview/4_wordVectorization/bert_representation.csv'\n","\n","# Save BERT representation to a CSV file\n","bert_representation.to_csv(csv_file_path, index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPjZm833qo+GKK6pwGrn/kJ","mount_file_id":"19DMbzXxVrYL3dLWHIfV5yaeS6lV9iz5k","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}